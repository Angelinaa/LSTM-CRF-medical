{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from model import BiLSTM_CRF\n",
    "import numpy as np\n",
    "import os, argparse, time, random\n",
    "from utils import str2bool, get_logger, get_entity\n",
    "from data import read_corpus, read_dictionary, tag2label, random_embedding,vocab_build\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#hypterparameters\n",
    "tf.flags.DEFINE_string('train_data','data_path','train data source')\n",
    "tf.flags.DEFINE_string('test_data', 'data_path', 'test data source')\n",
    "tf.flags.DEFINE_integer('batch_size', 16, 'sample of each minibatch')\n",
    "tf.flags.DEFINE_integer('epoch', 15, 'epoch of traing')\n",
    "tf.flags.DEFINE_integer('hidden_dim', 300, 'dim of hidden state')\n",
    "tf.flags.DEFINE_string('optimizer', 'Adam', 'Adam/Adadelta/Adagrad/RMSProp/Momentum/SG')\n",
    "tf.flags.DEFINE_boolean('CRF',True, 'use CRF at the top layer. if False, use Softmax')\n",
    "tf.flags.DEFINE_float('lr', 0.001, 'learing rate')\n",
    "tf.flags.DEFINE_float('clip', 5.0, 'gradient clipping')\n",
    "tf.flags.DEFINE_float('dropout', 0.5, 'dropout keep_prob')\n",
    "tf.flags.DEFINE_boolean('update_embeddings', True, 'update embeddings during traings')\n",
    "tf.flags.DEFINE_string('pretrain_embedding', 'random', 'use pretrained char embedding or init it randomly')\n",
    "tf.flags.DEFINE_integer('embedding_dim', 300, 'random init char embedding_dim')\n",
    "tf.flags.DEFINE_boolean('shuffle', True, 'shuffle training data before each epoch')\n",
    "tf.flags.DEFINE_string('mode', 'train', 'train/test/demo')\n",
    "tf.flags.DEFINE_string('demo_model', '1499785643', 'model for test and demo')\n",
    "tf.flags.DEFINE_string('wordPath', 'data_path/word', 'train/test/demo')\n",
    "args = tf.flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def create_voabulary(wordPath=args.wordPath):\n",
    "    cache_path = \"data_path/word_voabulary.pkl\"\n",
    "    #print(\"cache_path:\", cache_path, \"file_exists:\", os.path.exists(cache_path))\n",
    "    # load the cache file if exists\n",
    "    if os.path.exists(cache_path):\n",
    "        #print(cache_path)\n",
    "        with open(cache_path, 'rb') as data_f:\n",
    "            vocabulary_word2index, vocabulary_index2word = pickle.load(data_f)\n",
    "            return vocabulary_word2index, vocabulary_index2word\n",
    "    else:\n",
    "        vocabulary_word2index = {}\n",
    "        vocabulary_index2word = {}\n",
    "        words = open(wordPath).readlines()\n",
    "        print(\"vocabulary:\", len(words))\n",
    "        for i, vocab in enumerate(words):\n",
    "            vocabulary_word2index[vocab] = i + 1\n",
    "            vocabulary_index2word[i + 1] = vocab\n",
    "\n",
    "        # save to file system if vocabulary of words is not exists.\n",
    "        print(len(vocabulary_word2index))\n",
    "        if not os.path.exists(cache_path):\n",
    "            with open(cache_path, 'wb') as data_f:\n",
    "                pickle.dump((vocabulary_word2index, vocabulary_index2word), data_f)\n",
    "    return vocabulary_word2index, vocabulary_index2word\n",
    "#word2id, vocabulary_index2word = create_voabulary(wordPath=args.wordPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#vocab_build(\"data_path/word_voabulary.pkl\", \"data_path/all_data\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 4272\n"
     ]
    }
   ],
   "source": [
    "## get char embeddings\n",
    "word2id = read_dictionary(os.path.join('.', args.train_data, 'word2id.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if args.pretrain_embedding == 'random':\n",
    "    embeddings = random_embedding(word2id, args.embedding_dim)\n",
    "else:\n",
    "    embedding_path = 'pretrain_embedding.npy'\n",
    "    embeddings = np.array(np.load(embedding_path), dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data_path/train_data1 ./data_path/test_data1\n",
      "8312\n"
     ]
    }
   ],
   "source": [
    "if args.mode != 'demo':\n",
    "    train_path = os.path.join('.', args.train_data,'train_data1')\n",
    "    test_path = os.path.join('.', args.test_data, 'test_data1')\n",
    "    print(train_path, test_path)\n",
    "    '''\n",
    "    with open(train_path, encoding='utf-8') as fr:\n",
    "        lines = fr.readlines()\n",
    "    for line in lines:\n",
    "        line = line.replace(\"\\n\",\"\")\n",
    "        if line != '\\n':\n",
    "            #print(line)\n",
    "            if len(line.strip().split())==2:\n",
    "                [char, label] = line.strip().split()\n",
    "    '''\n",
    "    train_data = read_corpus(train_path)\n",
    "    test_data = read_corpus(test_path)\n",
    "    \n",
    "    test_size = len(train_data)\n",
    "    print(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.platform.flags._FlagValues object at 0x7f45fd7190f0>\n"
     ]
    }
   ],
   "source": [
    "## paths setting\n",
    "timestamp = str(int(time.time())) if args.mode == 'train' else args.demo_model\n",
    "output_path = os.path.join('.', args.train_data+\"_save\", timestamp)\n",
    "if not os.path.exists(output_path): os.makedirs(output_path)\n",
    "summary_path = os.path.join(output_path, \"summaries\")\n",
    "if not os.path.exists(summary_path): os.makedirs(summary_path)\n",
    "model_path = os.path.join(output_path, \"checkpoints/\")\n",
    "if not os.path.exists(model_path): os.makedirs(model_path)\n",
    "ckpt_prefix = os.path.join(model_path, \"model\")\n",
    "result_path = os.path.join(output_path, \"results\")\n",
    "if not os.path.exists(result_path): os.makedirs(result_path)\n",
    "log_path = os.path.join(result_path, \"log.txt\")\n",
    "get_logger(log_path).info(str(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========lr==== 0.001\n",
      "self.lr= 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data len= 8312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train lenght=8312 number_batches=130\n",
      "==========1 epoch begin train, time is 2018-01-24 15:11:12\n",
      "======seq length======64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 1 batch / 130 batches.\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-24 15:11:37 epoch 1, step 1, loss: 299.0, global_step: 1\n",
      "======seq length======64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing: 2 batch / 130 batches.\r"
     ]
    }
   ],
   "source": [
    "## training model\n",
    "if args.mode == 'train':\n",
    "    print(\"==========lr====\", args.lr)\n",
    "    model = BiLSTM_CRF(batch_size=args.batch_size, epoch_num=args.epoch, hidden_dim=args.hidden_dim, embeddings=embeddings,\n",
    "                       dropout_keep=args.dropout, optimizer=args.optimizer, lr=args.lr, clip_grad=args.clip,\n",
    "                       tag2label=tag2label, vocab=word2id, shuffle=args.shuffle,\n",
    "                       model_path=ckpt_prefix, summary_path=summary_path, log_path=log_path, result_path=result_path,\n",
    "                       CRF=args.CRF, update_embedding=args.update_embeddings)\n",
    "    \n",
    "    model.build_graph()\n",
    "    \n",
    "    print(\"train data len=\", len(train_data))\n",
    "    model.train(train_data, test_data)\n",
    "elif args.mode == 'test':\n",
    "    ckpt_file = tf.train.latest_checkpoint(model_path)\n",
    "    print(ckpt_file)\n",
    "    model = BiLSTM_CRF(batch_size=args.batch_size, epoch_num=args.epoch, hidden_dim=args.hidden_dim, embeddings=embeddings,\n",
    "                       dropout_keep=args.dropout, optimizer=args.optimizer, lr=args.lr, clip_grad=args.clip,\n",
    "                       tag2label=tag2label, vocab=word2id, shuffle=args.shuffle,\n",
    "                       model_path=ckpt_file, summary_path=summary_path, log_path=log_path, result_path=result_path,\n",
    "                       CRF=args.CRF, update_embedding=args.update_embedding)\n",
    "    model.build_graph()\n",
    "    print(\"test data: {}\".format(test_size))\n",
    "    model.test(test_data)\n",
    "elif args.mode == 'demo':\n",
    "    ckpt_file = tf.train.latest_checkpoint(model_path)\n",
    "    print(ckpt_file)\n",
    "    model = BiLSTM_CRF(batch_size=args.batch_size, epoch_num=args.epoch, hidden_dim=args.hidden_dim,\n",
    "                       embeddings=embeddings,\n",
    "                       dropout_keep=args.dropout, optimizer=args.optimizer, lr=args.lr, clip_grad=args.clip,\n",
    "                       tag2label=tag2label, vocab=word2id, shuffle=args.shuffle,\n",
    "                       model_path=ckpt_file, summary_path=summary_path, log_path=log_path, result_path=result_path,\n",
    "                       CRF=args.CRF, update_embedding=args.update_embeddings)\n",
    "    \n",
    "    model.build_graph()\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session as sess:\n",
    "        print(\"========demo===========\")\n",
    "        saver.restore(sess, ckpt_file)\n",
    "        \n",
    "        while(1):\n",
    "            print('Please input your sentence:')\n",
    "            demo_sent = input()\n",
    "            if demo_sent == '' or demo_sent.isspace():\n",
    "                print('See you next time!')\n",
    "                break\n",
    "            else:\n",
    "                demo_sent = list(demo_sent.strip())\n",
    "                demo_data = [(demo_sent, ['O'] * len(demo_sent))]\n",
    "                tag = model.demo_one(sess, demo_data)\n",
    "                PER, LOC, ORG = get_entity(tag, demo_sent)\n",
    "                print('PER: {}\\nLOC: {}\\nORG: {}'.format(PER, LOC, ORG))\n",
    "                                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
